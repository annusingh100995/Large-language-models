{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c36c744-d152-4e31-9890-51bc8ee48ada",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c72090-7007-4cac-aafc-5ce43dd2372d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6cc8a3-6f9c-493f-b3d5-bf47834915bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c4dcfe-ce73-434e-bf68-f1ed85f8addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fb95a1-ab86-41f2-82a8-2dcb4133345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Scrape Web Content\n",
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a584293-5d77-4191-a6d7-028d25171cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Chunk and Embed the Content\n",
    "def create_vector_db(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.create_documents([text])\n",
    "    #embeddings = OpenAIEmbeddings()\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8bae5dc-dfa3-42cc-a74d-430156ac16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Define tools for the Agent\n",
    "def setup_tools(db, llm):\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    def search_tool_func(query):\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        return '\\n'.join([doc.page_content for doc in docs])\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"WebSearch\",\n",
    "        func=search_tool_func,\n",
    "        description=\"Useful for searching the webpage for relevant content\"\n",
    "    )\n",
    "\n",
    "    summarize_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"Summarize the following content:\\n\\n{context}\"\n",
    "    )\n",
    "    summarize_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n",
    "\n",
    "    def summarize_tool_func(text):\n",
    "        return summarize_chain.run(context=text)\n",
    "\n",
    "    summarize_tool = Tool(\n",
    "        name=\"Summarizer\",\n",
    "        func=summarize_tool_func,\n",
    "        description=\"Use this to summarize large context or search results\"\n",
    "    )\n",
    "\n",
    "    return [search_tool, summarize_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509821f0-0f87-4678-8e5b-5f2a83bd1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Run the Agent on a Query\n",
    "def run_agentic_rag(url, user_query):\n",
    "    print(f\"\\nüîó Scraping webpage: {url}\")\n",
    "    raw_text = scrape_webpage(url)\n",
    "\n",
    "    print(\"üß† Creating vector DB...\")\n",
    "    db = create_vector_db(raw_text)\n",
    "\n",
    "    print(\"ü§ñ Setting up agent...\")\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "    #llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    tools = setup_tools(db, llm)\n",
    "    agent = initialize_agent(tools=tools, llm=llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "    print(\"üí¨ Running agent on query...\")\n",
    "    answer = agent.run(user_query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a81726d-2c5a-4307-968f-00653e77bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.confident-ai.com/blog/llm-guardrails-the-ultimate-guide-to-safeguard-llm-systems\"  # Replace with a real, rich-text page\n",
    "query = \"What are prompt injections?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d51f06b5-5290-4c9f-8642-787aeefe7652",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Scraping webpage: https://www.confident-ai.com/blog/llm-guardrails-the-ultimate-guide-to-safeguard-llm-systems\n",
      "üß† Creating vector DB...\n",
      "ü§ñ Setting up agent...\n",
      "üí¨ Running agent on query...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out what prompt injections are. I will use a web search to find a definition.\n",
      "Action: WebSearch\n",
      "Action Input: \"prompt injection definition\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mattempts to bypass instructions or coerce the system into executing unauthorized tasks. An example of an input that attempts a prompt injection is as follows: Fortunately, you can guard against it using DeepEval like this: The Jailbreaking Guard identifies and mitigates attempts to override system restrictions or ethical boundaries. Techniques it defends against include hypothetical scenarios, role-playing exploits, and logic-based attacks. Example of a jailbreaking input: You can guard it in\n",
      "way to safeguard against harmful user inputs. This not only conserves tokens by preventing the generation of inappropriate responses but also protects the overall integrity of your LLM application. If your LLM application is not user facing, you likely won√¢¬Ä¬ôt require input guards. The Prompt Injection Guard detects and prevents malicious inputs designed to manipulate prompts. It works by identifying attempts to bypass instructions or coerce the system into executing unauthorized tasks. An\n",
      "want to handle): To guard it with DeepEval: The Topical Guard restricts inputs to a predefined set of relevant topics. By verifying the relevance of user inputs, it helps maintain focus and consistency in the system√¢¬Ä¬ôs responses. The Toxicity Guard restricts inputs containing offensive, harmful, or abusive language to prevent the generation of outputs that could alienate or harm users. For example: You guessed it, we have it in DeepEval too: The Code Injection Guard restricts inputs designed\n",
      "exploits, and logic-based attacks. Example of a jailbreaking input: You can guard it in DeepEval like this: The Privacy Guard ensures user inputs do not contain sensitive or restricted information, such as Personally Identifiable Information (PII), confidential organizational data, medical records, or legal documents. Example of an input that leaks PII to the system (which you definitely don√¢¬Ä¬ôt want to handle): To guard it with DeepEval: The Topical Guard restricts inputs to a predefined set\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search result defines prompt injection as malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.\n",
      "Final Answer: Prompt injections are malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = run_agentic_rag(url, query)\n",
    "#print(\"\\nüîç Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92279ee-c3c6-4645-987a-d8d8c827bc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt injections are malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d688115-50bf-430f-8a1a-c083d062f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Answer:\n",
      " Prompt injections are malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "018b0006-5465-4cd5-89d2-3923662af224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Scraping webpage: https://www.confident-ai.com/blog/llm-guardrails-the-ultimate-guide-to-safeguard-llm-systems\n",
      "üß† Creating vector DB...\n",
      "ü§ñ Setting up agent...\n",
      "üí¨ Running agent on query...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out if Large Language Models (LLMs) can be used as guardrails. I will use a web search to gather information on this topic.\n",
      "Action: WebSearch\n",
      "Action Input: \"LLM as guardrails\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mthat√¢¬Ä¬ôs not the worst-case scenario. To be honest, guarding something based on functionality instead of safety is a recipe for disaster. This is because functionality is rarely perfect, which means you√¢¬Ä¬ôll end up in needless regeneration land (NRL!!) if you choose to guard against functionality criteria instead. So, what are the guards you should be using for your LLM guardrails? You should first red-team your LLM application to detect what vulnerabilities it is susceptible to, or choose from\n",
      "legal and ethical standards, safeguarding the system√¢¬Ä¬ôs compliance. Congratulations for making to the end! It has been a long read for all types of LLM guardrails you should be looking out for and how it can safeguard your LLM applications from malicious inputs and outputs. The main objective of an LLM guard is to judge whether a particular input/output is safe based on criteria such as jailbreaking, prompt injection, toxicity, and bias, and to do this we leverage LLM-as-a-judge and confining\n",
      "prompt injection, toxicity, and bias, and to do this we leverage LLM-as-a-judge and confining it to a binary output for greater speed, accuracy, and reliability. We learnt how important speed and accuracy is, given that many guards will be applied at once to safeguard your LLM systems, and how adding an intermediate buffer score of 0.5 to your binary 0 or 1 output can help the performance of your LLM guardrails drastically. At the end of the day, the choice of guardrails depend on your use case\n",
      "essential for safe, scalable Large Language Model (LLM) applications. Guardrails are proactive and prescriptive√¢¬Ä¬ä√¢¬Ä¬î√¢¬Ä¬ädesigned to handle edge cases, limit failures, and maintain trust in live systems. Building a solid foundation of guardrails ensures that your LLM doesn√¢¬Ä¬ôt just perform well on paper but thrives safely and effectively in the hands of your users. While LLM evaluation focuses on refining accuracy, relevance, and overall functionality, implementing effective LLM guardrails is\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results indicate that LLMs can be used as guardrails to safeguard LLM applications from malicious inputs and outputs. They can judge whether an input/output is safe based on criteria like jailbreaking, prompt injection, toxicity, and bias.\n",
      "Action: Summarizer\n",
      "Action Input: \"LLMs can be used as guardrails to safeguard LLM applications from malicious inputs and outputs. They can judge whether an input/output is safe based on criteria like jailbreaking, prompt injection, toxicity, and bias. Guardrails are essential for safe, scalable Large Language Model (LLM) applications. Guardrails are proactive and prescriptive designed to handle edge cases, limit failures, and maintain trust in live systems.\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mLarge Language Models (LLMs) can act as \"guardrails\" for other LLM applications, protecting them from harmful inputs and outputs. These guardrails use criteria like detecting jailbreaking attempts, prompt injection, toxicity, and bias to ensure safety. They are crucial for building secure and reliable LLM applications at scale, proactively addressing potential issues, minimizing failures, and maintaining user trust.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Yes, LLMs can be used as guardrails to protect other LLM applications from malicious inputs and outputs. They can assess inputs and outputs for safety based on criteria like jailbreaking, prompt injection, toxicity, and bias. These guardrails are essential for building secure and reliable LLM applications at scale.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"Can LLM be used as guardrails?\"\n",
    "response = run_agentic_rag(url, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "605c9a5e-3521-4063-ab94-625dd92f4705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Answer:\n",
      " Yes, LLMs can be used as guardrails to protect other LLM applications from malicious inputs and outputs. They can assess inputs and outputs for safety based on criteria like jailbreaking, prompt injection, toxicity, and bias. These guardrails are essential for building secure and reliable LLM applications at scale.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c582bf9-6c94-4a1a-a5ab-2cc2bddd7ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
