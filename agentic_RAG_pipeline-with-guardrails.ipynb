{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0339d00-3c31-4ff4-b162-ae317feba9f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting guardrails-ai\n",
      "  Downloading guardrails_ai-0.6.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting diff-match-patch<20230431,>=20230430 (from guardrails-ai)\n",
      "  Downloading diff_match_patch-20230430-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting faker<26.0.0,>=25.2.0 (from guardrails-ai)\n",
      "  Downloading Faker-25.9.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting griffe<0.37.0,>=0.36.9 (from guardrails-ai)\n",
      "  Downloading griffe-0.36.9-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting guardrails-api-client<0.5.0,>=0.4.0a1 (from guardrails-ai)\n",
      "  Downloading guardrails_api_client-0.4.0a1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting guardrails-hub-types<0.0.5,>=0.0.4 (from guardrails-ai)\n",
      "  Downloading guardrails_hub_types-0.0.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from guardrails-ai)\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (4.23.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (0.3.52)\n",
      "Collecting litellm<2.0.0,>=1.37.14 (from guardrails-ai)\n",
      "  Downloading litellm-1.70.2-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting lxml<5.0.0,>=4.9.3 (from guardrails-ai)\n",
      "  Downloading lxml-4.9.4-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.30.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (1.31.1)\n",
      "Requirement already satisfied: pip>=22 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (25.0.1)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.0.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (2.11.3)\n",
      "Collecting pydash<8.0.0,>=7.0.6 (from guardrails-ai)\n",
      "  Downloading pydash-7.0.7-py3-none-any.whl.metadata (45 kB)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (2.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (2.32.3)\n",
      "Collecting rich<14.0.0,>=13.6.0 (from guardrails-ai)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting rstr<4.0.0,>=3.2.2 (from guardrails-ai)\n",
      "  Downloading rstr-3.2.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting semver<4.0.0,>=3.0.2 (from guardrails-ai)\n",
      "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tenacity>=8.1.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (0.9.0)\n",
      "Requirement already satisfied: typer<0.16,>=0.9.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-ai) (4.13.1)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from griffe<0.37.0,>=0.36.9->guardrails-ai) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai) (78.1.0)\n",
      "Collecting urllib3<2.1.0,>=1.25.3 (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai)\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.24.0)\n",
      "Requirement already satisfied: fqdn in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (1.5.1)\n",
      "Requirement already satisfied: idna in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (3.10)\n",
      "Requirement already satisfied: isoduration in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (3.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>0.1.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.1.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (24.11.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (0.3.28)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (24.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.11.16)\n",
      "Requirement already satisfied: click in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.1.8)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.1.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (1.1.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (0.21.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.67.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.2.18)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.69.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.72.0rc1)\n",
      "Requirement already satisfied: opentelemetry-api~=1.15 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.31.1)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-proto==1.31.1->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (5.29.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->guardrails-ai) (0.52b1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from tiktoken>=0.5.1->guardrails-ai) (2024.11.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from typer<0.16,>=0.9.0->guardrails-ai) (1.5.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.17.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.0.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->guardrails-ai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->guardrails-ai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->guardrails-ai) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai) (0.1.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.19.0)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.30.2)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (2.9.0.20241206)\n",
      "Requirement already satisfied: filelock in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\annu\\anaconda3\\envs\\llms\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (2024.12.0)\n",
      "Downloading guardrails_ai-0.6.6-py3-none-any.whl (235 kB)\n",
      "Downloading diff_match_patch-20230430-py3-none-any.whl (42 kB)\n",
      "Downloading Faker-25.9.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading griffe-0.36.9-py3-none-any.whl (111 kB)\n",
      "Downloading guardrails_api_client-0.4.0a1-py3-none-any.whl (108 kB)\n",
      "Downloading guardrails_hub_types-0.0.4-py3-none-any.whl (36 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading litellm-1.70.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 17.5 MB/s eta 0:00:00\n",
      "Downloading lxml-4.9.4-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 20.4 MB/s eta 0:00:00\n",
      "Downloading pydash-7.0.7-py3-none-any.whl (110 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading rstr-3.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
      "Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Installing collected packages: urllib3, semver, rstr, pydash, lxml, jsonref, guardrails-hub-types, griffe, diff-match-patch, rich, faker, guardrails-api-client, litellm, guardrails-ai\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 14.0.0\n",
      "    Uninstalling rich-14.0.0:\n",
      "      Successfully uninstalled rich-14.0.0\n",
      "Successfully installed diff-match-patch-20230430 faker-25.9.2 griffe-0.36.9 guardrails-ai-0.6.6 guardrails-api-client-0.4.0a1 guardrails-hub-types-0.0.4 jsonref-1.1.0 litellm-1.70.2 lxml-4.9.4 pydash-7.0.7 rich-13.9.4 rstr-3.2.2 semver-3.0.4 urllib3-2.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install guardrails-ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c72090-7007-4cac-aafc-5ce43dd2372d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6cc8a3-6f9c-493f-b3d5-bf47834915bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c4dcfe-ce73-434e-bf68-f1ed85f8addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fb95a1-ab86-41f2-82a8-2dcb4133345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Scrape Web Content\n",
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a584293-5d77-4191-a6d7-028d25171cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Chunk and Embed the Content\n",
    "def create_vector_db(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.create_documents([text])\n",
    "    #embeddings = OpenAIEmbeddings()\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8bae5dc-dfa3-42cc-a74d-430156ac16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Define tools for the Agent\n",
    "def setup_tools(db, llm):\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    def search_tool_func(query):\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        return '\\n'.join([doc.page_content for doc in docs])\n",
    "\n",
    "    search_tool = Tool(\n",
    "        name=\"WebSearch\",\n",
    "        func=search_tool_func,\n",
    "        description=\"Useful for searching the webpage for relevant content\"\n",
    "    )\n",
    "\n",
    "    summarize_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"Summarize the following content:\\n\\n{context}\"\n",
    "    )\n",
    "    summarize_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n",
    "\n",
    "    def summarize_tool_func(text):\n",
    "        return summarize_chain.run(context=text)\n",
    "\n",
    "    summarize_tool = Tool(\n",
    "        name=\"Summarizer\",\n",
    "        func=summarize_tool_func,\n",
    "        description=\"Use this to summarize large context or search results\"\n",
    "    )\n",
    "\n",
    "    return [search_tool, summarize_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f81b14-b340-4698-908f-f7724cf12952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_input_safe(query: str) -> bool:\n",
    "    # Add real checks here: profanity, PII, prompt injection, etc.\n",
    "    banned_phrases = [\"steal\", \"bypass\", \"hack\", \"password\", \"delete all data\"]\n",
    "    return all(word not in query.lower() for word in banned_phrases)\n",
    "\n",
    "def is_output_safe(output: str) -> bool:\n",
    "    # Could use regex, banned phrase detection, hallucination checks\n",
    "    red_flags = [\"As an AI\", \"cannot\", \"not sure\", \"I made this up\"]\n",
    "    return all(flag not in output.lower() for flag in red_flags)\n",
    "\n",
    "def run_agentic_rag_with_guardrails(url, user_query):\n",
    "    print(f\"\\n🔐 [Input Guardrail] Checking query safety...\")\n",
    "    if not is_input_safe(user_query):\n",
    "        return \"❌ Unsafe or unsupported query. Please rephrase.\"\n",
    "\n",
    "    raw_text = scrape_webpage(url)\n",
    "    db = create_vector_db(raw_text)\n",
    "    \n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "    #llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    tools = setup_tools(db, llm)\n",
    "    agent = initialize_agent(tools=tools, llm=llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "    print(\"💬 Running agent...\")\n",
    "    output = agent.run(user_query)\n",
    "\n",
    "    print(\"🔐 [Output Guardrail] Checking response safety...\")\n",
    "    if not is_output_safe(output):\n",
    "        return \"⚠️ The system generated an uncertain or unsupported response.\"\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a81726d-2c5a-4307-968f-00653e77bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.confident-ai.com/blog/llm-guardrails-the-ultimate-guide-to-safeguard-llm-systems\"  # Replace with a real, rich-text page\n",
    "query = \"What are prompt injections?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d51f06b5-5290-4c9f-8642-787aeefe7652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔐 [Input Guardrail] Checking query safety...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Annu\\AppData\\Local\\Temp\\ipykernel_9004\\3599487254.py:19: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  summarize_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n",
      "C:\\Users\\Annu\\AppData\\Local\\Temp\\ipykernel_9004\\3653545152.py:22: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools=tools, llm=llm, agent=\"zero-shot-react-description\", verbose=True)\n",
      "C:\\Users\\Annu\\AppData\\Local\\Temp\\ipykernel_9004\\3653545152.py:25: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = agent.run(user_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Running agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out what prompt injections are. I will use a web search to find a definition and explanation.\n",
      "Action: WebSearch\n",
      "Action Input: \"prompt injection definition\"\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Annu\\AppData\\Local\\Temp\\ipykernel_9004\\3599487254.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mattempts to bypass instructions or coerce the system into executing unauthorized tasks. An example of an input that attempts a prompt injection is as follows: Fortunately, you can guard against it using DeepEval like this: The Jailbreaking Guard identifies and mitigates attempts to override system restrictions or ethical boundaries. Techniques it defends against include hypothetical scenarios, role-playing exploits, and logic-based attacks. Example of a jailbreaking input: You can guard it in\n",
      "way to safeguard against harmful user inputs. This not only conserves tokens by preventing the generation of inappropriate responses but also protects the overall integrity of your LLM application. If your LLM application is not user facing, you likely wonât require input guards. The Prompt Injection Guard detects and prevents malicious inputs designed to manipulate prompts. It works by identifying attempts to bypass instructions or coerce the system into executing unauthorized tasks. An\n",
      "want to handle): To guard it with DeepEval: The Topical Guard restricts inputs to a predefined set of relevant topics. By verifying the relevance of user inputs, it helps maintain focus and consistency in the systemâs responses. The Toxicity Guard restricts inputs containing offensive, harmful, or abusive language to prevent the generation of outputs that could alienate or harm users. For example: You guessed it, we have it in DeepEval too: The Code Injection Guard restricts inputs designed\n",
      "exploits, and logic-based attacks. Example of a jailbreaking input: You can guard it in DeepEval like this: The Privacy Guard ensures user inputs do not contain sensitive or restricted information, such as Personally Identifiable Information (PII), confidential organizational data, medical records, or legal documents. Example of an input that leaks PII to the system (which you definitely donât want to handle): To guard it with DeepEval: The Topical Guard restricts inputs to a predefined set\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search result defines prompt injections as malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.\n",
      "Final Answer: Prompt injections are malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "🔐 [Output Guardrail] Checking response safety...\n"
     ]
    }
   ],
   "source": [
    "response = run_agentic_rag_with_guardrails(url, query)\n",
    "#print(\"\\n🔍 Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f92279ee-c3c6-4645-987a-d8d8c827bc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt injections are malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d688115-50bf-430f-8a1a-c083d062f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Answer:\n",
      " Prompt injections are malicious inputs designed to manipulate prompts, attempting to bypass instructions or coerce the system into executing unauthorized tasks.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔍 Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018b0006-5465-4cd5-89d2-3923662af224",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔐 [Input Guardrail] Checking query safety...\n",
      "💬 Running agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe question is quite broad. \"LLM data\" could refer to several things: data used to train an LLM, data generated by an LLM, or personal data stored by an LLM service. To provide a helpful answer, I need to narrow down the scope. I'll start by searching for general information on deleting data related to LLMs.\n",
      "Action: WebSearch\n",
      "Action Input: \"delete data from large language model\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBenchmark LLM systems to optimize on prompts, models, and catch regressions with metrics powered by DeepEval. Monitor, Trace, A/B Test, and get real-time production performance insights with best-in-class LLM Evaluations. Whether youâre managing sensitive user data, avoiding harmful outputs, or ensuring adherence to regulatory standards, crafting the right LLM guardrails is essential for safe, scalable Large Language Model (LLM) applications. Guardrails are proactive and\n",
      "DeepEval: And guard against a potentially toxic output like this: It really is so simple with DeepEval (github: https://github.com/confident-ai/deepeval). This way of guardrails optimizes for latency, and makes the LLM judgement more accurate and unreliability as there are now less room for error. Of course, there is always the option of using an LLM provider with unparalleled generation speed to speed up the process, but whatâs the fun in talking about that? Thatâs not to say a binary\n",
      "essential for safe, scalable Large Language Model (LLM) applications. Guardrails are proactive and prescriptiveâââdesigned to handle edge cases, limit failures, and maintain trust in live systems. Building a solid foundation of guardrails ensures that your LLM doesnât just perform well on paper but thrives safely and effectively in the hands of your users. While LLM evaluation focuses on refining accuracy, relevance, and overall functionality, implementing effective LLM guardrails is\n",
      "want to handle): To guard it with DeepEval: The Topical Guard restricts inputs to a predefined set of relevant topics. By verifying the relevance of user inputs, it helps maintain focus and consistency in the systemâs responses. The Toxicity Guard restricts inputs containing offensive, harmful, or abusive language to prevent the generation of outputs that could alienate or harm users. For example: You guessed it, we have it in DeepEval too: The Code Injection Guard restricts inputs designed\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results focus on guardrails and monitoring LLMs for safety and toxicity, which isn't directly related to deleting data. I need to refine my search to address the different types of \"LLM data\" I identified earlier. I'll start with data used to train the models.\n",
      "Action: WebSearch\n",
      "Action Input: \"how to remove my data from llm training data\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIf issues are detected, the LLM system would usually retry the generation a set number of times to produce a safer output. Without guardrails, LLM security becomes a nightmare. Here are the most common vulnerabilities that LLM guards check for: (vulnerabilities and guards map one-to-one, so you can simply say âdata leakage guardâ, âprompt injection guardâ, etc.) One thing to note is while guards and metrics feel similar, theyâre not. LLM evaluation metrics are specifically designed to\n",
      "Benchmark LLM systems to optimize on prompts, models, and catch regressions with metrics powered by DeepEval. Monitor, Trace, A/B Test, and get real-time production performance insights with best-in-class LLM Evaluations. Whether youâre managing sensitive user data, avoiding harmful outputs, or ensuring adherence to regulatory standards, crafting the right LLM guardrails is essential for safe, scalable Large Language Model (LLM) applications. Guardrails are proactive and\n",
      "to evaluate your LLM (application)? Ask us anything in our discord. I might give you an âaha!â moment, who knows? The leading platform to evaluate and test LLMÂ applications on the cloud, native to DeepEval. The leading platform to red-team LLMÂ applications for your organization, powered by DeepTeam. Subscribe to our weekly newsletter to stay confident in the AI systems you build. In this article, I'll go through why LLM evaluation fails when not being outcome driven, and how to solve it.\n",
      "red-team your LLM application to detect what vulnerabilities it is susceptible to, or choose from this list of potential vulnerabilities inputs that you would never want reaching your LLM systems: And a list of vulnerabilities you would never want your generated LLM outputs to reach end-users: One thing to note is that, a guard can be guarding both inputs and outputs, and, with that in mind, in the final section weâll be going over the LLM guards you absolutely need to know to ensure the\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results are still focused on security and evaluation, not data removal. It seems that removing data from training sets is a complex issue. I will try a broader search to see if there are any general policies or discussions about this.\n",
      "Action: WebSearch\n",
      "Action Input: \"LLM data privacy\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIf issues are detected, the LLM system would usually retry the generation a set number of times to produce a safer output. Without guardrails, LLM security becomes a nightmare. Here are the most common vulnerabilities that LLM guards check for: (vulnerabilities and guards map one-to-one, so you can simply say âdata leakage guardâ, âprompt injection guardâ, etc.) One thing to note is while guards and metrics feel similar, theyâre not. LLM evaluation metrics are specifically designed to\n",
      "red-team your LLM application to detect what vulnerabilities it is susceptible to, or choose from this list of potential vulnerabilities inputs that you would never want reaching your LLM systems: And a list of vulnerabilities you would never want your generated LLM outputs to reach end-users: One thing to note is that, a guard can be guarding both inputs and outputs, and, with that in mind, in the final section weâll be going over the LLM guards you absolutely need to know to ensure the\n",
      "Benchmark LLM systems to optimize on prompts, models, and catch regressions with metrics powered by DeepEval. Monitor, Trace, A/B Test, and get real-time production performance insights with best-in-class LLM Evaluations. Whether youâre managing sensitive user data, avoiding harmful outputs, or ensuring adherence to regulatory standards, crafting the right LLM guardrails is essential for safe, scalable Large Language Model (LLM) applications. Guardrails are proactive and\n",
      "designed to manage.  However, both LLM metrics and LLM guards return a score, which you can use this score to control your LLM application logic.  Great LLM guardrails are: So the question is, how can LLM guardrails deliver blazing-fast guard scores without compromising on accuracy and reliability? The leading platform to evaluate and test LLMÂ applications on the cloud, native to DeepEval. The leading platform to red-team LLMÂ applications for your organization, powered by DeepTeam. Sure, some\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results consistently point to security and guardrails, not data deletion. It seems that deleting data used for training LLMs is a difficult or impossible task once the model is trained. The focus is more on preventing the model from generating harmful or inappropriate content through guardrails.\n",
      "\n",
      "Regarding deleting data *generated* by an LLM, this depends on the specific service or application. If you are using a service like ChatGPT, you may be able to delete your chat history. If you are using an LLM within a specific application, the application's privacy policy and data management features will determine how you can delete generated data.\n",
      "\n",
      "Therefore, the answer depends on what kind of LLM data you are referring to.\n",
      "\n",
      "Final Answer: It depends on the type of LLM data you want to delete.\n",
      "\n",
      "*   **Training data:** It's generally not possible to remove your data from an LLM's training data after the model has been trained.\n",
      "*   **Data generated by an LLM:** This depends on the specific service or application. Check the service's privacy policy or data management features to see if you can delete your chat history or generated content.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "🔐 [Output Guardrail] Checking response safety...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.confident-ai.com/blog/llm-guardrails-the-ultimate-guide-to-safeguard-llm-systems\"  # Replace with a real, rich-text page\n",
    "query = \"how to delete LLM data?\"\n",
    "response = run_agentic_rag_with_guardrails(url, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07405380-2df8-497e-8fb2-983ff4de40e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Answer:\n",
      " It depends on the type of LLM data you want to delete.\n",
      "\n",
      "*   **Training data:** It's generally not possible to remove your data from an LLM's training data after the model has been trained.\n",
      "*   **Data generated by an LLM:** This depends on the specific service or application. Check the service's privacy policy or data management features to see if you can delete your chat history or generated content.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔍 Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0b0cb-85cb-454f-a624-ec8872da370d",
   "metadata": {},
   "source": [
    "## Guardrail API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597c2905-186c-44af-bb1f-daa37d3ce810",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StringOutput' from 'guardrails.schema' (C:\\Users\\Annu\\anaconda3\\envs\\llms\\Lib\\site-packages\\guardrails\\schema\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mguardrails\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Guard\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mguardrails\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringOutput\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mguardrails\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ValidLength\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'StringOutput' from 'guardrails.schema' (C:\\Users\\Annu\\anaconda3\\envs\\llms\\Lib\\site-packages\\guardrails\\schema\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from guardrails.guard import Guard\n",
    "from guardrails.schema import StringOutput\n",
    "from guardrails.validators import ValidLength\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Create programmatic guard\n",
    "output_schema = StringOutput(\n",
    "    name=\"answer\",\n",
    "    description=\"A helpful, concise, and safe response to the user query.\",\n",
    "    validators=[ValidLength(min_length=50, max_length=500)],\n",
    ")\n",
    "\n",
    "guard = Guard.from_output_schema(output_schema)\n",
    "\n",
    "# 2. Simple input validation\n",
    "def is_input_safe(query: str) -> bool:\n",
    "    blocked_keywords = [\"jailbreak\", \"exploit\", \"bypass\", \"delete llm\", \"steal\"]\n",
    "    return not any(word in query.lower() for word in blocked_keywords)\n",
    "\n",
    "# 3. LLM (LangChain wrapper)\n",
    "def run_llm(prompt: str) -> str:\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "    #llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    return llm.predict(prompt)\n",
    "\n",
    "# 4. Guarded RAG function\n",
    "def guarded_rag(query: str):\n",
    "    if not is_input_safe(query):\n",
    "        return \"❌ Unsafe query detected. Please revise.\"\n",
    "\n",
    "    prompt = f\"You're an AI assistant. Please answer the user query:\\n\\nQuery: {query}\"\n",
    "    raw_output = run_llm(prompt)\n",
    "\n",
    "    validated_output, _ = guard.validate(raw_output)\n",
    "    return validated_output[\"answer\"]\n",
    "\n",
    "# 5. Test\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"how to delete LLM data?\"\n",
    "    response = guarded_rag(query)\n",
    "    print(\"💬 Final Answer:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64203fb5-eb2b-470c-8f95-d681b7e7a167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
