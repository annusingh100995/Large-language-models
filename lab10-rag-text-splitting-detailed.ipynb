{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe026b0-1c61-472e-94a5-bc5f02fd7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.text_splitter import ( CharacterTextSplitter,\n",
    "RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter,\n",
    "TextSplitter,TokenTextSplitter)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66454345-d0de-40db-9ba4-3adf92596f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a0bc0b-7a3f-40dd-9c27-81e66b471243",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = \"./\"\n",
    "file_path = \"./frankenstein.txt\"\n",
    "db_dir = os.path.join(current_dir,\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82da914-a4e7-4149-8ece-96e25c5eef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists \n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"The file {file_path} does not exist.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eea037a-ab5d-426d-87bf-4ef21bef6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content from the file \n",
    "loader = TextLoader(file_path, encoding=\"utf8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7ff74b-b7be-47a9-a016-fce96b36eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Defining the embedding\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe2c5b4-55c5-417a-ae28-c2bd942f333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and persist vertor store\n",
    "def create_vector_store(docs,store_name):\n",
    "    persistent_directory = os.path.join(db_dir,store_name)\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(f\"\\n --- Creating vector store {store_name} ---\")\n",
    "        db = Chroma.from_documents(\n",
    "            docs , embeddings, persist_directory=persistent_directory\n",
    "        )\n",
    "        print(f\" --- Finished creating vector store {store_name} ---\")\n",
    "    else:\n",
    "        print(\n",
    "            f\" Vector store {store_name} already exists.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4031611-5adc-4043-900c-1bf982c331b7",
   "metadata": {},
   "source": [
    " 1. Character based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d65ebe-4daa-466d-8f36-00d982223ce2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2200, which is longer than the specified 1000\n",
      "Created a chunk of size 1782, which is longer than the specified 1000\n",
      "Created a chunk of size 2255, which is longer than the specified 1000\n",
      "Created a chunk of size 1060, which is longer than the specified 1000\n",
      "Created a chunk of size 1497, which is longer than the specified 1000\n",
      "Created a chunk of size 1243, which is longer than the specified 1000\n",
      "Created a chunk of size 1493, which is longer than the specified 1000\n",
      "Created a chunk of size 1112, which is longer than the specified 1000\n",
      "Created a chunk of size 1614, which is longer than the specified 1000\n",
      "Created a chunk of size 1201, which is longer than the specified 1000\n",
      "Created a chunk of size 1201, which is longer than the specified 1000\n",
      "Created a chunk of size 1105, which is longer than the specified 1000\n",
      "Created a chunk of size 1216, which is longer than the specified 1000\n",
      "Created a chunk of size 1339, which is longer than the specified 1000\n",
      "Created a chunk of size 1260, which is longer than the specified 1000\n",
      "Created a chunk of size 1007, which is longer than the specified 1000\n",
      "Created a chunk of size 1344, which is longer than the specified 1000\n",
      "Created a chunk of size 1907, which is longer than the specified 1000\n",
      "Created a chunk of size 1103, which is longer than the specified 1000\n",
      "Created a chunk of size 1524, which is longer than the specified 1000\n",
      "Created a chunk of size 2317, which is longer than the specified 1000\n",
      "Created a chunk of size 1621, which is longer than the specified 1000\n",
      "Created a chunk of size 1693, which is longer than the specified 1000\n",
      "Created a chunk of size 1017, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 2166, which is longer than the specified 1000\n",
      "Created a chunk of size 1135, which is longer than the specified 1000\n",
      "Created a chunk of size 1103, which is longer than the specified 1000\n",
      "Created a chunk of size 1617, which is longer than the specified 1000\n",
      "Created a chunk of size 1410, which is longer than the specified 1000\n",
      "Created a chunk of size 2042, which is longer than the specified 1000\n",
      "Created a chunk of size 1434, which is longer than the specified 1000\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1417, which is longer than the specified 1000\n",
      "Created a chunk of size 1053, which is longer than the specified 1000\n",
      "Created a chunk of size 1220, which is longer than the specified 1000\n",
      "Created a chunk of size 1002, which is longer than the specified 1000\n",
      "Created a chunk of size 1164, which is longer than the specified 1000\n",
      "Created a chunk of size 1151, which is longer than the specified 1000\n",
      "Created a chunk of size 1440, which is longer than the specified 1000\n",
      "Created a chunk of size 1091, which is longer than the specified 1000\n",
      "Created a chunk of size 1319, which is longer than the specified 1000\n",
      "Created a chunk of size 1138, which is longer than the specified 1000\n",
      "Created a chunk of size 1599, which is longer than the specified 1000\n",
      "Created a chunk of size 1259, which is longer than the specified 1000\n",
      "Created a chunk of size 1472, which is longer than the specified 1000\n",
      "Created a chunk of size 1045, which is longer than the specified 1000\n",
      "Created a chunk of size 1462, which is longer than the specified 1000\n",
      "Created a chunk of size 1202, which is longer than the specified 1000\n",
      "Created a chunk of size 1015, which is longer than the specified 1000\n",
      "Created a chunk of size 1707, which is longer than the specified 1000\n",
      "Created a chunk of size 1380, which is longer than the specified 1000\n",
      "Created a chunk of size 1117, which is longer than the specified 1000\n",
      "Created a chunk of size 1153, which is longer than the specified 1000\n",
      "Created a chunk of size 1433, which is longer than the specified 1000\n",
      "Created a chunk of size 1337, which is longer than the specified 1000\n",
      "Created a chunk of size 1544, which is longer than the specified 1000\n",
      "Created a chunk of size 1461, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1151, which is longer than the specified 1000\n",
      "Created a chunk of size 1013, which is longer than the specified 1000\n",
      "Created a chunk of size 1119, which is longer than the specified 1000\n",
      "Created a chunk of size 1488, which is longer than the specified 1000\n",
      "Created a chunk of size 1355, which is longer than the specified 1000\n",
      "Created a chunk of size 1264, which is longer than the specified 1000\n",
      "Created a chunk of size 1225, which is longer than the specified 1000\n",
      "Created a chunk of size 1302, which is longer than the specified 1000\n",
      "Created a chunk of size 1083, which is longer than the specified 1000\n",
      "Created a chunk of size 1035, which is longer than the specified 1000\n",
      "Created a chunk of size 1592, which is longer than the specified 1000\n",
      "Created a chunk of size 1177, which is longer than the specified 1000\n",
      "Created a chunk of size 1995, which is longer than the specified 1000\n",
      "Created a chunk of size 1277, which is longer than the specified 1000\n",
      "Created a chunk of size 1140, which is longer than the specified 1000\n",
      "Created a chunk of size 1249, which is longer than the specified 1000\n",
      "Created a chunk of size 1423, which is longer than the specified 1000\n",
      "Created a chunk of size 1051, which is longer than the specified 1000\n",
      "Created a chunk of size 1039, which is longer than the specified 1000\n",
      "Created a chunk of size 1130, which is longer than the specified 1000\n",
      "Created a chunk of size 1504, which is longer than the specified 1000\n",
      "Created a chunk of size 1376, which is longer than the specified 1000\n",
      "Created a chunk of size 1226, which is longer than the specified 1000\n",
      "Created a chunk of size 1021, which is longer than the specified 1000\n",
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1347, which is longer than the specified 1000\n",
      "Created a chunk of size 1076, which is longer than the specified 1000\n",
      "Created a chunk of size 1261, which is longer than the specified 1000\n",
      "Created a chunk of size 1289, which is longer than the specified 1000\n",
      "Created a chunk of size 1312, which is longer than the specified 1000\n",
      "Created a chunk of size 1212, which is longer than the specified 1000\n",
      "Created a chunk of size 1066, which is longer than the specified 1000\n",
      "Created a chunk of size 1489, which is longer than the specified 1000\n",
      "Created a chunk of size 1502, which is longer than the specified 1000\n",
      "Created a chunk of size 1576, which is longer than the specified 1000\n",
      "Created a chunk of size 1659, which is longer than the specified 1000\n",
      "Created a chunk of size 1138, which is longer than the specified 1000\n",
      "Created a chunk of size 1488, which is longer than the specified 1000\n",
      "Created a chunk of size 1386, which is longer than the specified 1000\n",
      "Created a chunk of size 1184, which is longer than the specified 1000\n",
      "Created a chunk of size 1045, which is longer than the specified 1000\n",
      "Created a chunk of size 1132, which is longer than the specified 1000\n",
      "Created a chunk of size 1674, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Using Chracter-based Spliiting --\n",
      "\n",
      " --- Creating vector store chroma_db_char ---\n",
      " --- Finished creating vector store chroma_db_char ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Character based Splitting\n",
    "# Splits text into chunks based on a specified number of characters\n",
    "# Useful for consistent chunks sizes regardless of content structure\n",
    "\n",
    "print(\"\\n -- Using Chracter-based Spliiting --\")\n",
    "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "char_docs = char_splitter.split_documents(documents)\n",
    "create_vector_store(char_docs,\"chroma_db_char\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c0928-cace-4d36-8f66-f59537af452d",
   "metadata": {},
   "source": [
    "2. Sentence-based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23611b5f-09d2-4f7e-9711-c12f51154e63",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Using sentence based Splitting -- \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa551073c4e477d96e806bde8c95f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Annu\\anaconda3\\envs\\llms\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Annu\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2dc2a70b4b4f64b133f2e4ea091eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cccb91a51a447fab3f3cac6d82ee00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7340e98149c48a28c6bc8badde5e014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a07892adeb4a66a9aea1c05c5d2657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7041eb874d45bc91879ce389cae869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0ace11b4044280a9c9c4ec29396c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5420e2e43e4b3483c9a901d0947da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c8e2197499466cb21adfe390ce76a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2120daff3233432384bc804db92bc6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4540c111c9b2486681d229c2a8576de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Creating vector store chroma_db_sent ---\n",
      " --- Finished creating vector store chroma_db_sent ---\n"
     ]
    }
   ],
   "source": [
    "# Splits text into chunks based on sentences, ensuring chunks end at\n",
    "# sentence boundaries. Ideal for maintaining semantic coherence within chunk\n",
    "print(\"\\n -- Using sentence based Splitting -- \")\n",
    "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
    "sent_docs = sent_splitter.split_documents(documents)\n",
    "create_vector_store(sent_docs,\"chroma_db_sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c78e3e-cb78-42a2-bf67-77307baba2ec",
   "metadata": {},
   "source": [
    "3. Token-based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d793d6e7-4be7-419c-b737-3c58cda29902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Token based Splitting -- \n",
      "\n",
      " --- Creating vector store chroma_db_token ---\n",
      " --- Finished creating vector store chroma_db_token ---\n"
     ]
    }
   ],
   "source": [
    "# Splits text into chunks based on tokens (words/sub-words)\n",
    "# Useful for transformer model with strict token limits\n",
    "print(\"\\n -- Token based Splitting -- \")\n",
    "token_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=56)\n",
    "token_docs = token_splitter.split_documents(documents)\n",
    "create_vector_store(token_docs,\"chroma_db_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb29d1a-d008-4980-bd27-31a28608e5f8",
   "metadata": {},
   "source": [
    " 4. Recursive Character-based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2828a256-4edc-4621-a1b7-a796a45010b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Recursive Character based Splitting --\n",
      "\n",
      " --- Creating vector store chroma_db_rec_char ---\n",
      " --- Finished creating vector store chroma_db_rec_char ---\n"
     ]
    }
   ],
   "source": [
    "# Attempts to split text at natural boundaries (sentences/paragraphs) within character limit\n",
    "# Balances b/w maintaining coherence and adhering to character limit\n",
    "print(\"\\n -- Recursive Character based Splitting --\")\n",
    "rec_char_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
    "create_vector_store(rec_char_docs,\"chroma_db_rec_char\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4da668-06dc-422e-a5bd-7feca960da17",
   "metadata": {},
   "source": [
    " 5. Custom Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4583a020-14a0-482e-aa93-83d3f82189e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Using Custom Splitting -- \n",
      "\n",
      " --- Creating vector store chroma_db_custom ---\n",
      " --- Finished creating vector store chroma_db_custom ---\n"
     ]
    }
   ],
   "source": [
    "# Allows creating custom splitting logic.\n",
    "# Useful for documents with unique structure that standard splitters can't handle\n",
    "print(\"\\n -- Using Custom Splitting -- \")\n",
    "class CustomTextSplitter(TextSplitter):\n",
    "    def split_text(self,text):\n",
    "        return text.split(\"\\n\\n\")\n",
    "\n",
    "\n",
    "custom_splitter = CustomTextSplitter()\n",
    "custome_docs = custom_splitter.split_documents(documents)\n",
    "create_vector_store(custome_docs,\"chroma_db_custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdc8f2cc-dd03-4e45-a35a-032f75ee3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query a vector store\n",
    "def query_vector_store(store_name,query):\n",
    "    persistent_directory = os.path.join(db_dir,store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n -- Querying the Vector Store {store_name} -- \")\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory, \n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        retriever = db.as_retriever(\n",
    "            search_type = \"similarity\",\n",
    "            search_kwargs={\"k\":5}\n",
    "        )\n",
    "        relavant_docs = retriever.invoke(query)\n",
    "        #Results\n",
    "        print(f\"\\n -- Relevant Documents for {store_name} -- \")\n",
    "        for i, doc in enumerate(relavant_docs,1):\n",
    "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "            if doc.metadata:\n",
    "                print(f\"Source : {doc.metadata.get('source','Unknown')}\\n\")\n",
    "    else:\n",
    "        print(f\"Vector store {store_name} does  not exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f3c45f-59d1-4305-a605-ec360188a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did Juliet die?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf1c4890-c42b-43d8-92f0-7f0e6657b5c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Querying the Vector Store chroma_db_char -- \n",
      "\n",
      " -- Relevant Documents for chroma_db_char -- \n",
      "Document 1:\n",
      "Thus the poor sufferer tried to comfort others and herself. She indeed\n",
      "gained the resignation she desired. But I, the true murderer, felt the\n",
      "never-dying worm alive in my bosom, which allowed of no hope or\n",
      "consolation. Elizabeth also wept and was unhappy, but hers also was\n",
      "the misery of innocence, which, like a cloud that passes over the fair\n",
      "moon, for a while hides but cannot tarnish its brightness. Anguish and\n",
      "despair had penetrated into the core of my heart; I bore a hell within\n",
      "me which nothing could extinguish. We stayed several hours with\n",
      "Justine, and it was with great difficulty that Elizabeth could tear\n",
      "herself away. “I wish,” cried she, “that I were to die with you; I\n",
      "cannot live in this world of misery.”\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 2:\n",
      "Justine assumed an air of cheerfulness, while she with difficulty\n",
      "repressed her bitter tears. She embraced Elizabeth and said in a voice\n",
      "of half-suppressed emotion, “Farewell, sweet lady, dearest Elizabeth,\n",
      "my beloved and only friend; may heaven, in its bounty, bless and\n",
      "preserve you; may this be the last misfortune that you will ever\n",
      "suffer! Live, and be happy, and make others so.”\n",
      "\n",
      "And on the morrow Justine died. Elizabeth’s heart-rending eloquence\n",
      "failed to move the judges from their settled conviction in the\n",
      "criminality of the saintly sufferer. My passionate and indignant\n",
      "appeals were lost upon them. And when I received their cold answers\n",
      "and heard the harsh, unfeeling reasoning of these men, my purposed\n",
      "avowal died away on my lips. Thus I might proclaim myself a madman,\n",
      "but not revoke the sentence passed upon my wretched victim. She\n",
      "perished on the scaffold as a murderess!\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 3:\n",
      "The appearance of Justine was calm. She was dressed in mourning, and\n",
      "her countenance, always engaging, was rendered, by the solemnity of her\n",
      "feelings, exquisitely beautiful. Yet she appeared confident in\n",
      "innocence and did not tremble, although gazed on and execrated by\n",
      "thousands, for all the kindness which her beauty might otherwise have\n",
      "excited was obliterated in the minds of the spectators by the\n",
      "imagination of the enormity she was supposed to have committed. She\n",
      "was tranquil, yet her tranquillity was evidently constrained; and as\n",
      "her confusion had before been adduced as a proof of her guilt, she\n",
      "worked up her mind to an appearance of courage. When she entered the\n",
      "court she threw her eyes round it and quickly discovered where we were\n",
      "seated. A tear seemed to dim her eye when she saw us, but she quickly\n",
      "recovered herself, and a look of sorrowful affection seemed to attest\n",
      "her utter guiltlessness.\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 4:\n",
      "Justine shook her head mournfully. “I do not fear to die,” she said;\n",
      "“that pang is past. God raises my weakness and gives me courage to\n",
      "endure the worst. I leave a sad and bitter world; and if you remember\n",
      "me and think of me as of one unjustly condemned, I am resigned to the\n",
      "fate awaiting me. Learn from me, dear lady, to submit in patience to\n",
      "the will of heaven!”\n",
      "\n",
      "During this conversation I had retired to a corner of the prison room,\n",
      "where I could conceal the horrid anguish that possessed me. Despair!\n",
      "Who dared talk of that? The poor victim, who on the morrow was to pass\n",
      "the awful boundary between life and death, felt not, as I did, such\n",
      "deep and bitter agony. I gnashed my teeth and ground them together,\n",
      "uttering a groan that came from my inmost soul. Justine started. When\n",
      "she saw who it was, she approached me and said, “Dear sir, you are very\n",
      "kind to visit me; you, I hope, do not believe that I am guilty?”\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 5:\n",
      "Justine was called on for her defence. As the trial had proceeded, her\n",
      "countenance had altered. Surprise, horror, and misery were strongly\n",
      "expressed. Sometimes she struggled with her tears, but when she was\n",
      "desired to plead, she collected her powers and spoke in an audible\n",
      "although variable voice.\n",
      "\n",
      "“God knows,” she said, “how entirely I am innocent. But I\n",
      "do not pretend that my protestations should acquit me; I rest my innocence\n",
      "on a plain and simple explanation of the facts which have been adduced\n",
      "against me, and I hope the character I have always borne will incline my\n",
      "judges to a favourable interpretation where any circumstance appears\n",
      "doubtful or suspicious.”\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_vector_store(\"chroma_db_char\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1f731c9-c3d1-4673-bbf7-8679edf3d94f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Querying the Vector Store chroma_db_custom -- \n",
      "\n",
      " -- Relevant Documents for chroma_db_custom -- \n",
      "Document 1:\n",
      "Thus the poor sufferer tried to comfort others and herself. She indeed\n",
      "gained the resignation she desired. But I, the true murderer, felt the\n",
      "never-dying worm alive in my bosom, which allowed of no hope or\n",
      "consolation. Elizabeth also wept and was unhappy, but hers also was\n",
      "the misery of innocence, which, like a cloud that passes over the fair\n",
      "moon, for a while hides but cannot tarnish its brightness. Anguish and\n",
      "despair had penetrated into the core of my heart; I bore a hell within\n",
      "me which nothing could extinguish. We stayed several hours with\n",
      "Justine, and it was with great difficulty that Elizabeth could tear\n",
      "herself away. “I wish,” cried she, “that I were to die with you; I\n",
      "cannot live in this world of misery.”\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 2:\n",
      "Justine shook her head mournfully. “I do not fear to die,” she said;\n",
      "“that pang is past. God raises my weakness and gives me courage to\n",
      "endure the worst. I leave a sad and bitter world; and if you remember\n",
      "me and think of me as of one unjustly condemned, I am resigned to the\n",
      "fate awaiting me. Learn from me, dear lady, to submit in patience to\n",
      "the will of heaven!”\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 3:\n",
      "The appearance of Justine was calm. She was dressed in mourning, and\n",
      "her countenance, always engaging, was rendered, by the solemnity of her\n",
      "feelings, exquisitely beautiful. Yet she appeared confident in\n",
      "innocence and did not tremble, although gazed on and execrated by\n",
      "thousands, for all the kindness which her beauty might otherwise have\n",
      "excited was obliterated in the minds of the spectators by the\n",
      "imagination of the enormity she was supposed to have committed. She\n",
      "was tranquil, yet her tranquillity was evidently constrained; and as\n",
      "her confusion had before been adduced as a proof of her guilt, she\n",
      "worked up her mind to an appearance of courage. When she entered the\n",
      "court she threw her eyes round it and quickly discovered where we were\n",
      "seated. A tear seemed to dim her eye when she saw us, but she quickly\n",
      "recovered herself, and a look of sorrowful affection seemed to attest\n",
      "her utter guiltlessness.\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 4:\n",
      "Sweet and beloved Elizabeth! I read and reread her letter, and some\n",
      "softened feelings stole into my heart and dared to whisper paradisiacal\n",
      "dreams of love and joy; but the apple was already eaten, and the\n",
      "angel’s arm bared to drive me from all hope. Yet I would die to make\n",
      "her happy. If the monster executed his threat, death was inevitable; yet,\n",
      "again, I considered whether my marriage would hasten my fate. My\n",
      "destruction might indeed arrive a few months sooner, but if my torturer\n",
      "should suspect that I postponed it, influenced by his menaces, he would\n",
      "surely find other and perhaps more dreadful means of revenge. He had vowed\n",
      "_to be with me on my wedding-night_, yet he did not consider that\n",
      "threat as binding him to peace in the meantime, for as if to show me that\n",
      "he was not yet satiated with blood, he had murdered Clerval immediately\n",
      "after the enunciation of his threats. I resolved, therefore, that if my\n",
      "immediate union with my cousin would conduce either to hers or my\n",
      "father’s happiness, my adversary’s designs against my life\n",
      "should not retard it a single hour.\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n",
      "Document 5:\n",
      "I listened to this discourse with the extremest agony. I, not in deed,\n",
      "but in effect, was the true murderer. Elizabeth read my anguish in my\n",
      "countenance, and kindly taking my hand, said, “My dearest friend, you\n",
      "must calm yourself. These events have affected me, God knows how\n",
      "deeply; but I am not so wretched as you are. There is an expression of\n",
      "despair, and sometimes of revenge, in your countenance that makes me\n",
      "tremble. Dear Victor, banish these dark passions. Remember the\n",
      "friends around you, who centre all their hopes in you. Have we lost\n",
      "the power of rendering you happy? Ah! While we love, while we are\n",
      "true to each other, here in this land of peace and beauty, your native\n",
      "country, we may reap every tranquil blessing—what can disturb our\n",
      "peace?”\n",
      "\n",
      "Source : ./frankenstein.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_vector_store(\"chroma_db_custom\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc168c-78ae-4924-ad48-74a311b11e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
